{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Trailblazer** ðŸ”¥\n",
    "\n",
    "**Trailblazer is a course recommender that uses the Naive Bayes machine learning algorithm to recommend CS classes to CS students based on classes they have taken before. In order to do this, it uses course evaluations and details about course content to intelligently recommend classes to students given classes they have taken and liked**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation\n",
    "\n",
    "### **Acquiring Data**\n",
    "\n",
    "### Course Evaluations\n",
    "\n",
    "I was able to obtain course evaluations for each class on \n",
    "[Stanford's course evaluations website](https://evals.stanford.edu/new-course-evaluation-system?utm_source=pocket_saves). \n",
    "I decided to take 4 questions from each course evaluation. Those questions were\n",
    "\n",
    "- How well did you achieve the learning goals of this course?\n",
    "- How much did you learn from this course?\n",
    "- Overall, how would you describe the quality of the instruction in this course?\n",
    "- How organized was this course?\n",
    "\n",
    "I thought these question would best represent how likely someone who had taken class was to report that they enjoyed the class.\n",
    "\n",
    "All of these questions had 5 options with which a student, who had taken the class, could choose from and the course evaluation document \n",
    "for a class listed the percentage of students who chose a specifc option.\n",
    "\n",
    "To get the data in a computer readable format, I manually typed in the percentages for all CS classes for each question for each option.\n",
    "In order to condense these percentages into a z-value for each class, that I could throw into a $\\sigma$ function to get a probability value, \n",
    "I did the following:\n",
    "\n",
    "As you can see, for a particular class, I just append the probability values to a vector then add that vector to a matrix.\n",
    "\n",
    "```python \n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "root_dir = f\"PATH\"\n",
    "\n",
    "with open(\"prob-full-matrix\", \"x+\") as file_write:\n",
    "    with open(\"prob-full-matrix\", 'w') as file_write:\n",
    "        for root, dirs, files in os.walk(root_dir):\n",
    "            for dir in dirs:\n",
    "                file_write.write(dir)\n",
    "                for idx in range(4):\n",
    "                    file_path = root_dir + \"/\" + dir + \"/\" + str(idx) + \"-\" + dir\n",
    "                    with open(file_path, \"r\") as curr_file:\n",
    "                        file_write.write(\",\" + curr_file.read().replace(\"\\n\", \",\"))\n",
    "\n",
    "                file_write.write(\"\\n\")\n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "Next, I used PCA to condense the each class vector into a z value which would be plugged into a $\\sigma$ function to obtain a probability:\n",
    "\n",
    "It turned out that the first principle component explained 99.64% of the variance between class vectors, so in order to get the first\n",
    "principle component's linear combination of each of the questions option percentage, I took the dot product of each class vector and \n",
    "each the first principle component, the result of which I used as a z-value for the probability that a student who takes a class likes\n",
    "a class.\n",
    "\n",
    "\n",
    "```python \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "df = pd.read_csv(\"./prob-full-matrix\", index_col=0, names=np.arange(0, 20, 1, dtype=int))\n",
    "PROB_MATRIX = df.to_numpy()\n",
    "\n",
    "for i in range(PROB_MATRIX.shape[1]):\n",
    "    mean = np.sum(PROB_MATRIX[:, i]) / PROB_MATRIX.shape[1]\n",
    "    PROB_MATRIX[:, i] = PROB_MATRIX[:, i] - mean\n",
    "\n",
    "COV_MATRIX = (1/(PROB_MATRIX.shape[0]-1)) * (PROB_MATRIX.T @ PROB_MATRIX)\n",
    "U, S, V = np.linalg.svd(COV_MATRIX)\n",
    "\n",
    "print(f\"{S[0]/np.sum(S)}% variance is exaplained by the first principal component.\")\n",
    "\n",
    "first_pc = U[:, 1]\n",
    "first_pc\n",
    "relative_probs = (first_pc.reshape(1, 20) @ PROB_MATRIX.T)\n",
    "dict = {\"Class\":df.index.to_numpy(), \"Z\": relative_probs.flatten()}\n",
    "z_df = pd.DataFrame(dict)\n",
    "\n",
    "def sig(z):\n",
    "    return 1 / (1 + np.exp(-z))\n",
    "\n",
    "dict = {\"Class\":df.index.to_numpy(), \"Prob\": sig(relative_probs.flatten())}\n",
    "\n",
    "prob_df = pd.DataFrame(dict)\n",
    "z_df.to_csv(\"z_like.csv\", index=False)\n",
    "```\n",
    "\n",
    "### Simulating CS Population\n",
    "\n",
    "I was unable to get anonymized transcripts or any other data from Stanford University, so I was forced to simulate a population.\n",
    "To do this I manually put Stanford CS classes into 14 buckets:\n",
    "\n",
    "\n",
    "- Art\n",
    "- Computer Engineering\n",
    "- Theory\n",
    "- Artificial Intelligence\n",
    "- Algorithms\n",
    "- Computational Biology\n",
    "- Graphics\n",
    "- Human Computer Interaction\n",
    "- Information\n",
    "- Systems\n",
    "- Software Engineering\n",
    "- Robotics\n",
    "- Ethics\n",
    "- Not Specific\n",
    "- Cyber Security\n",
    "\n",
    "To create the population, I first made a copy of the z-value which would eventually represent the probability the current member\n",
    "of the population would like a class. I then sampled 1 value from the uniform multinomial distribution that is the 14 buckets and \n",
    "based on that I gave all classes that were in the same category a bias that would increase the probability that that particular person \n",
    "would like classes in the cateogry. Then for that particular person, I sampled from a bernoulli distribution for each class. The result\n",
    "of each iteration was a vector of 0s and 1s representing whether the current member would like a class or not. I appended this to a population\n",
    "matrix and repeated the aforementioned steps 17000 times.\n",
    "\n",
    "Here is a snippet for some details\n",
    "\n",
    "```python\n",
    "for usr_idx in range(POP_SIZE):\n",
    "    user_pref_cat = np.random.choice(cat_nums)\n",
    "    user_probs = z_file.copy(deep = True)\n",
    "\n",
    "    for key, value in cat_dict.items():\n",
    "        if(np.any(np.in1d(np.array(sing_to_many(user_pref_cat)), cat_dict[key]))):\n",
    "            user_probs.loc[key,\"Z\"] = user_probs.loc[key,\"Z\"] + PREF_BIAS\n",
    "        else:\n",
    "            user_probs.loc[key,\"Z\"] = user_probs.loc[key,\"Z\"] - NOT_PREF_BIAS\n",
    "\n",
    "    user_probs[\"Z\"] = sig(user_probs[\"Z\"])\n",
    "\n",
    "    user_vec = np.array([int(np.random.binomial(1, p)) for p in user_probs[\"Z\"]])\n",
    "\n",
    "    POP_MATRIX[usr_idx, :] = user_vec\n",
    "```\n",
    "\n",
    "### **Training**\n",
    "\n",
    "To train, from my newly acquired population matrix of 0s and 1s, I picked a class to be the truth value and removed it from the population matrix. The new\n",
    "population matrix was deemed the training data I then calculated the parameters i.e. $(P(X_i \\mid Y = 1) \\text{ and } P(X_i \\mid Y = 0))$, according to Naive Bayes, \n",
    "and appended them to two paramter matricesâ€”one representing of $X_i$ if $Y = 1$ and the other represent $Y = 0$. At this point, I had two matrices with the \n",
    "following structure: \n",
    "\n",
    "$PARAMMATRIXYone_{i,j}$ represents the probability that a student would like class $X_j$ given that they like $X_i$\n",
    "\n",
    "The other matrix represents the opposite. that is:\n",
    "\n",
    "$PARAMMATRIXYzero_{i,j}$ represents the probability that a student would like class $X_j$ given that they did not like $X_i$\n",
    "\n",
    "\n",
    "### **Prediction**\n",
    "\n",
    "To give suggestions to a student based on classes that they reported that they like, I looped over all classes that they had \n",
    "not taken and calculated the probability that they would like the class given the classes that they said they like using the \n",
    "naive bayes assumption. I did the same for whether they would not like the class they haven't taken before. I then combined both\n",
    "value into a log odds and appended it to a list. After looping, I sorted the log odds and reported ones that were the largest and\n",
    "smallest.\n",
    "\n",
    "\n",
    "### To try it\n",
    "\n",
    "I didn't have enough time to make a website, so the code is available [here](https://github.com/jayson1200/trailblazer.git).\n",
    "To run it, all you have to do is change all of the paths at the top of the file to correct paths for your system and run\n",
    "lass-recommender.py the Algorithm folder via python in an environment that has pandas and numpy. If you have any suggestions, \n",
    "please contact me at jmeribe@stanford.edu. I am very interested in improving recommendations.\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
